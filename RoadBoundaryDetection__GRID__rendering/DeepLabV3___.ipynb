{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import tensorflow as tf\n",
    "\n",
    "def SqueezeAndExcite(inputs, ratio=8):\n",
    "    init = inputs\n",
    "    filters = init.shape[-1]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    x = init * se\n",
    "    return x\n",
    "\n",
    "def ASPP(inputs):\n",
    "    \"\"\" Image Pooling \"\"\"\n",
    "    shape = inputs.shape\n",
    "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
    "    y1 = Conv2D(256, 1, padding=\"same\", use_bias=False)(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation(\"relu\")(y1)\n",
    "    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n",
    "\n",
    "    \"\"\" 1x1 conv \"\"\"\n",
    "    y2 = Conv2D(256, 1, padding=\"same\", use_bias=False)(inputs)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y2 = Activation(\"relu\")(y2)\n",
    "\n",
    "    \"\"\" 3x3 conv rate=6 \"\"\"\n",
    "    y3 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=6)(inputs)\n",
    "    y3 = BatchNormalization()(y3)\n",
    "    y3 = Activation(\"relu\")(y3)\n",
    "\n",
    "    \"\"\" 3x3 conv rate=12 \"\"\"\n",
    "    y4 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=12)(inputs)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Activation(\"relu\")(y4)\n",
    "\n",
    "    \"\"\" 3x3 conv rate=18 \"\"\"\n",
    "    y5 = Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=18)(inputs)\n",
    "    y5 = BatchNormalization()(y5)\n",
    "    y5 = Activation(\"relu\")(y5)\n",
    "\n",
    "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
    "    y = Conv2D(256, 1, padding=\"same\", use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def deeplabv3_plus(shape):\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    encoder = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs)\n",
    "\n",
    "    image_features = encoder.get_layer(\"conv4_block6_out\").output\n",
    "    x_a = ASPP(image_features)\n",
    "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
    "\n",
    "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
    "    x_b = Conv2D(filters=48, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
    "    x_b = BatchNormalization()(x_b)\n",
    "    x_b = Activation('relu')(x_b)\n",
    "\n",
    "    x = Concatenate()([x_a, x_b])\n",
    "    x = SqueezeAndExcite(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SqueezeAndExcite(x)\n",
    "\n",
    "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
    "    x = Conv2D(1, 1)(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = deeplabv3_plus((512, 512, 3))\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dheeraj710/DataPreprocessing___directMasking__.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "  smooth=1\n",
    "  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "  union = K.sum(y_true,[1,2,3])+K.sum(y_true,[1,2,3])-intersection\n",
    "  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "  return iou\n",
    "\n",
    "def iou_loss(y_true, y_pred):\n",
    "    smooth = 1e-15  # Small constant to prevent division by zero\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return 1. - iou\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_true, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision, Accuracy\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\"\"\" Global parameters \"\"\"\n",
    "__PATH__ = \"/content/\"\n",
    "__PATH__dataSet = \"/content/DataPreprocessing___directMasking__/CORREC____/new_data\"\n",
    "IMG_SIZE = 512\n",
    "H = IMG_SIZE\n",
    "W = IMG_SIZE\n",
    "\n",
    "\"\"\" Creating a directory \"\"\"\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def shuffling(x, y):\n",
    "    x, y = shuffle(x, y, random_state=42)\n",
    "    return x, y\n",
    "\n",
    "def load_data(path):\n",
    "    x = sorted(glob(os.path.join(path, \"image\", \"*png\")))\n",
    "    y = sorted(glob(os.path.join(path, \"mask\", \"*png\")))\n",
    "    return x, y\n",
    "\n",
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch=2):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(10)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    \"\"\" Directory for storing files \"\"\"\n",
    "    create_dir(__PATH__ + \"/files/\")\n",
    "\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    batch_size = 4\n",
    "    lr = 1e-4  # 1e-4\n",
    "    num_epochs = 100\n",
    "    model_path = os.path.join(\"files\", \"model.h5\")\n",
    "    csv_path = os.path.join(\"files\", \"data.csv\")\n",
    "\n",
    "    \"\"\" Dataset \"\"\"\n",
    "    dataset_path = __PATH__dataSet\n",
    "    train_path = os.path.join(dataset_path, \"train\")\n",
    "    valid_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "    print(\"DEBUG -->\", train_path)\n",
    "    print(\"DEBUG --> \", valid_path)\n",
    "\n",
    "    train_x, train_y = load_data(train_path)\n",
    "    train_x, train_y = shuffling(train_x, train_y)\n",
    "    valid_x, valid_y = load_data(valid_path)\n",
    "\n",
    "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
    "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
    "\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = deeplabv3_plus((H, W, 3))\n",
    "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=[dice_coef, iou, Recall(), Precision(), Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        TensorBoard(),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n",
    "    ]\n",
    "\n",
    "    model_history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
